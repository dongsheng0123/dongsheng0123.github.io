<!DOCTYPE html><html lang="en-gb"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>#06 Stable Diffusion - PORTFOLIO</title><meta name="robots" content="noindex,nofollow"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="alternate" type="application/atom+xml" href="https://dongsheng0123.github.io/feed.xml"><link rel="alternate" type="application/json" href="https://dongsheng0123.github.io/feed.json"><meta property="og:title" content="#06 Stable Diffusion"><meta property="og:image" content="https://dongsheng0123.github.io/media/posts/5/387336446-7ccaf2c1-9b72-41ae-9a89-5688c94b7abe.png"><meta property="og:image:width" content="1634"><meta property="og:image:height" content="964"><meta property="og:site_name" content="PORTFOLIO"><meta property="og:description" content="这算是个火了两年的话题了。Midjourney入门易，Stable Diffusion则更能生成想要的效果（特别是工作流模式ComfyUI真的可以算得上日常能帮得上一点忙的）。 不过相比今年初的DeepSeek带来的极致的算力利用效率提高，stable diffusion就显得平淡许多，从SD1.5到SDXL，再到Flux.1和SD3.5，需要的算力被一步步拔高。算力还是其次，主要是对于显存的要求实在是太高了，16G显存仅仅只是入门。 不过这跟我这种部署在云端使用的，倒是关系不大，2块钱一个小时的4090我还是用得起的。🤣 总之就先分享下我认为的几个很有用的工作流（我日常都是使用ComfyUI，所以下面基本都是ComfyUI的截图，本地/云端部署就略过啦，有需要的可以找我帮忙），工作流都是我亲手搭建的哦。 第一个就是最基础的文生图啦，只要输入你想要的场景描述，就能生成对于的图片。而我这个有点不一样的是，我串联了一个我自己炼制的浴室柜lora（微调模型），它能使我生成的图片里带有我想要的这个浴室柜产品。 炼制产品lora对于一些复杂的场景（比如人与物体有交互）的出图很有帮助。 这个浴室柜lora的训练，难点就在于，如何让模型明白，镜子的存在，主要是打标的时候多试几次就可以了。 第二个我推荐的就是万物迁徙的工作流（这个名字我起的，有点二），主要是利用了Flux Fill和Redux模型，两者结合能做出如下图的效果。 你有可能想到的事，其实这个工作流，可以用一张产品图，一张场景图，直接生成产品在对应场景里面的图片，这是可以的。 就很适合做成一键试穿，一键更换家居等等。 不过只能做一些简单的，不涉及太多人与物体交互的图片，而且因为Flux Redux模型是走condition通道的，它出来的细节不一定和原来的能完全对应上。比如说上面的换衣服，和模特坐到椅子上，仔细看都有微妙的区别（不认真看倒也看不出来）。 所以如果是复杂场景，建议还是训练lora。 第三个毛坯生成场景图，这个比较贴近我的日常工作来着，但实话说，它的可控性不如上面两个，先看图吧。 如图，你只需要一个毛坯房照片，一个你想要风格的参考图，就能生成该毛坯按风格装修之后的大致效果。实话说，很利于谈单子，二十秒钟就能出一个图。 但其实更适合概念阶段使用，如果你想房间的布局和软硬装完全按照你的想法来，就会花费比较多的时间。 只要一张毛坯照片，就能看到装修好的大致效果，这才是它的舒适区。&hellip;"><meta property="og:url" content="https://dongsheng0123.github.io/stable-diffusion.html"><meta property="og:type" content="article"><link rel="stylesheet" href="https://dongsheng0123.github.io/assets/css/style.css?v=7b86bf805c84cea57658efdc5e8f58dc"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://dongsheng0123.github.io/stable-diffusion.html"},"headline":"#06 Stable Diffusion","datePublished":"2025-03-11T20:12+08:00","dateModified":"2025-03-16T10:57+08:00","image":{"@type":"ImageObject","url":"https://dongsheng0123.github.io/media/posts/5/387336446-7ccaf2c1-9b72-41ae-9a89-5688c94b7abe.png","height":964,"width":1634},"description":"这算是个火了两年的话题了。Midjourney入门易，Stable Diffusion则更能生成想要的效果（特别是工作流模式ComfyUI真的可以算得上日常能帮得上一点忙的）。 不过相比今年初的DeepSeek带来的极致的算力利用效率提高，stable diffusion就显得平淡许多，从SD1.5到SDXL，再到Flux.1和SD3.5，需要的算力被一步步拔高。算力还是其次，主要是对于显存的要求实在是太高了，16G显存仅仅只是入门。 不过这跟我这种部署在云端使用的，倒是关系不大，2块钱一个小时的4090我还是用得起的。🤣 总之就先分享下我认为的几个很有用的工作流（我日常都是使用ComfyUI，所以下面基本都是ComfyUI的截图，本地/云端部署就略过啦，有需要的可以找我帮忙），工作流都是我亲手搭建的哦。 第一个就是最基础的文生图啦，只要输入你想要的场景描述，就能生成对于的图片。而我这个有点不一样的是，我串联了一个我自己炼制的浴室柜lora（微调模型），它能使我生成的图片里带有我想要的这个浴室柜产品。 炼制产品lora对于一些复杂的场景（比如人与物体有交互）的出图很有帮助。 这个浴室柜lora的训练，难点就在于，如何让模型明白，镜子的存在，主要是打标的时候多试几次就可以了。 第二个我推荐的就是万物迁徙的工作流（这个名字我起的，有点二），主要是利用了Flux Fill和Redux模型，两者结合能做出如下图的效果。 你有可能想到的事，其实这个工作流，可以用一张产品图，一张场景图，直接生成产品在对应场景里面的图片，这是可以的。 就很适合做成一键试穿，一键更换家居等等。 不过只能做一些简单的，不涉及太多人与物体交互的图片，而且因为Flux Redux模型是走condition通道的，它出来的细节不一定和原来的能完全对应上。比如说上面的换衣服，和模特坐到椅子上，仔细看都有微妙的区别（不认真看倒也看不出来）。 所以如果是复杂场景，建议还是训练lora。 第三个毛坯生成场景图，这个比较贴近我的日常工作来着，但实话说，它的可控性不如上面两个，先看图吧。 如图，你只需要一个毛坯房照片，一个你想要风格的参考图，就能生成该毛坯按风格装修之后的大致效果。实话说，很利于谈单子，二十秒钟就能出一个图。 但其实更适合概念阶段使用，如果你想房间的布局和软硬装完全按照你的想法来，就会花费比较多的时间。 只要一张毛坯照片，就能看到装修好的大致效果，这才是它的舒适区。&hellip;","author":{"@type":"Person","name":"刘东升","url":"https://dongsheng0123.github.io/liudongsheng/liu-dong-sheng/"},"publisher":{"@type":"Organization","name":"刘东升"}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="post-template"><header class="header" id="js-header"><a href="https://dongsheng0123.github.io/" class="logo">PORTFOLIO</a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"><li><a href="https://dongsheng0123.github.io/guan-yu-wo-2.html" target="_self">关于我</a></li><li><a href="https://dongsheng0123.github.io/fang-an.html" target="_self">作品选集</a></li><li><a href="https://dongsheng0123.github.io/ming-xiang-cang.html" target="_self">冥想舱</a></li><li><a href="https://dongsheng0123.github.io/pian-qie-xi-lie.html" target="_self">片切系列</a></li><li><a href="https://dongsheng0123.github.io/rong-xue-cha-tai.html" target="_self">融雪茶台</a></li><li><a href="https://dongsheng0123.github.io/gong-yi-zheng-li-yan-jiu.html" target="_self">工艺研究</a></li><li class="active"><a href="https://dongsheng0123.github.io/stable-diffusion.html" target="_self">SD绘图</a></li><li><a href="https://dongsheng0123.github.io/ai-ide-zi-dong-bian-xie-cheng-xu.html" target="_self">AI编程</a></li></ul></nav></header><main class="post"><div class="wrapper"><article class="content"><header class="content__header"><a href="https://dongsheng0123.github.io/tags/agi/" class="content__maintag">AGI</a><h1 class="content__title">#06 Stable Diffusion</h1><div class="content__meta"><div class="content__author">By <a href="https://dongsheng0123.github.io/liudongsheng/liu-dong-sheng/" class="invert" rel="author" title="刘东升">刘东升</a></div></div></header><figure class="content__featured-image"><img src="https://dongsheng0123.github.io/media/posts/5/387336446-7ccaf2c1-9b72-41ae-9a89-5688c94b7abe.png" srcset="https://dongsheng0123.github.io/media/posts/5/responsive/387336446-7ccaf2c1-9b72-41ae-9a89-5688c94b7abe-xs.png 300w, https://dongsheng0123.github.io/media/posts/5/responsive/387336446-7ccaf2c1-9b72-41ae-9a89-5688c94b7abe-sm.png 480w, https://dongsheng0123.github.io/media/posts/5/responsive/387336446-7ccaf2c1-9b72-41ae-9a89-5688c94b7abe-md.png 768w, https://dongsheng0123.github.io/media/posts/5/responsive/387336446-7ccaf2c1-9b72-41ae-9a89-5688c94b7abe-lg.png 1200w" sizes="(min-width: 56.25em) 100vw, (min-width: 37.5em) 50vw, 100vw" loading="eager" height="964" width="1634" alt=""></figure><div class="content__inner"><div class="content__entry"><p>这算是个火了两年的话题了。Midjourney入门易，Stable Diffusion则更能生成想要的效果（特别是工作流模式ComfyUI真的可以算得上日常能帮得上一点忙的）。</p><p>不过相比今年初的DeepSeek带来的极致的算力利用效率提高，stable diffusion就显得平淡许多，从SD1.5到SDXL，再到Flux.1和SD3.5，需要的算力被一步步拔高。算力还是其次，主要是对于显存的要求实在是太高了，16G显存仅仅只是入门。</p><p>不过这跟我这种部署在云端使用的，倒是关系不大，2块钱一个小时的4090我还是用得起的。🤣</p><p>总之就先分享下我认为的几个很有用的工作流（我日常都是使用ComfyUI，所以下面基本都是ComfyUI的截图，本地/云端部署就略过啦，有需要的可以找我帮忙），工作流都是我亲手搭建的哦。</p><figure class="post__image post__image--center"><img loading="lazy" src="https://dongsheng0123.github.io/media/posts/5/Wei-Xin-Tu-Pian_2025-03-11_194121_267.jpg" height="746" width="1440" alt="" sizes="(max-width: 48em) 100vw, 100vw" srcset="https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_194121_267-xs.jpg 300w, https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_194121_267-sm.jpg 480w, https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_194121_267-md.jpg 768w"><figcaption>这是基本的文生图，但有点不一样的是我用了自己炼制的Lora</figcaption></figure><p>第一个就是最基础的文生图啦，只要输入你想要的场景描述，就能生成对于的图片。而我这个有点不一样的是，我串联了一个我自己炼制的浴室柜lora（微调模型），它能使我生成的图片里带有我想要的这个浴室柜产品。</p><p>炼制产品lora对于一些复杂的场景（比如人与物体有交互）的出图很有帮助。</p><figure class="post__image post__image--center"><img loading="lazy" src="https://dongsheng0123.github.io/media/posts/5/Wei-Xin-Tu-Pian_2025-03-11_194443_261.jpg" height="1985" width="3830" alt="" sizes="(max-width: 48em) 100vw, 100vw" srcset="https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_194443_261-xs.jpg 300w, https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_194443_261-sm.jpg 480w, https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_194443_261-md.jpg 768w"><figcaption>我用了18张不同角度的产品图片</figcaption></figure><figure class="post__image post__image--center"><img loading="lazy" src="https://dongsheng0123.github.io/media/posts/5/Wei-Xin-Tu-Pian_2025-03-11_194447_139.jpg" height="767" width="1470" alt="" sizes="(max-width: 48em) 100vw, 100vw" srcset="https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_194447_139-xs.jpg 300w, https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_194447_139-sm.jpg 480w, https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_194447_139-md.jpg 768w"><figcaption>这是本地的电脑，训练了10个小时</figcaption></figure><p>这个浴室柜lora的训练，难点就在于，如何让模型明白，镜子的存在，主要是打标的时候多试几次就可以了。</p><p>第二个我推荐的就是万物迁徙的工作流（这个名字我起的，有点二），主要是利用了Flux Fill和Redux模型，两者结合能做出如下图的效果。</p><figure class="post__image post__image--center"><img loading="lazy" src="https://dongsheng0123.github.io/media/posts/5/Wei-Xin-Tu-Pian_2025-03-11_195223_869.jpg" height="746" width="1440" alt="" sizes="(max-width: 48em) 100vw, 100vw" srcset="https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_195223_869-xs.jpg 300w, https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_195223_869-sm.jpg 480w, https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_195223_869-md.jpg 768w"><figcaption>换衣工作流，把上面的JK的水手服换给下面的OL</figcaption></figure><figure class="post__image post__image--center"><img loading="lazy" src="https://dongsheng0123.github.io/media/posts/5/Wei-Xin-Tu-Pian_2025-03-11_195350_020.png" height="1985" width="3830" alt="" sizes="(max-width: 48em) 100vw, 100vw" srcset="https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_195350_020-xs.png 300w, https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_195350_020-sm.png 480w, https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_195350_020-md.png 768w"><figcaption>这张则是让男模特坐在原本空无一物的单人椅上</figcaption></figure><p>你有可能想到的事，其实这个工作流，可以用一张产品图，一张场景图，直接生成产品在对应场景里面的图片，这是可以的。</p><p>就很适合做成一键试穿，一键更换家居等等。</p><p>不过只能做一些简单的，不涉及太多人与物体交互的图片，而且因为Flux Redux模型是走condition通道的，它出来的细节不一定和原来的能完全对应上。比如说上面的换衣服，和模特坐到椅子上，仔细看都有微妙的区别（不认真看倒也看不出来）。</p><p>所以如果是复杂场景，建议还是训练lora。</p><p>第三个毛坯生成场景图，这个比较贴近我的日常工作来着，但实话说，它的可控性不如上面两个，先看图吧。</p><figure class="post__image post__image--center"><img loading="lazy" src="https://dongsheng0123.github.io/media/posts/5/Wei-Xin-Tu-Pian_2025-03-11_200430_026.jpg" height="1985" width="3830" alt="" sizes="(max-width: 48em) 100vw, 100vw" srcset="https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_200430_026-xs.jpg 300w, https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_200430_026-sm.jpg 480w, https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-11_200430_026-md.jpg 768w"><figcaption>毛坯生成场景图工作流</figcaption></figure><p>如图，你只需要一个毛坯房照片，一个你想要风格的参考图，就能生成该毛坯按风格装修之后的大致效果。实话说，很利于谈单子，二十秒钟就能出一个图。</p><p>但其实更适合概念阶段使用，如果你想房间的布局和软硬装完全按照你的想法来，就会花费比较多的时间。</p><p>只要一张毛坯照片，就能看到装修好的大致效果，这才是它的舒适区。</p><p>当然还有好多好玩的工作流，比如很实用的产品精修（就是去瑕疵），短视频物品/人物/画风转化，后面看到好玩的我也会及时更新上来，就期待着呗。</p><figure class="post__image post__image--center"><img loading="lazy" src="https://dongsheng0123.github.io/media/posts/5/Wei-Xin-Tu-Pian_2025-03-16_103946_314.png" height="1988" width="3830" alt="" sizes="(max-width: 48em) 100vw, 100vw" srcset="https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-16_103946_314-xs.png 300w, https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-16_103946_314-sm.png 480w, https://dongsheng0123.github.io/media/posts/5/responsive/Wei-Xin-Tu-Pian_2025-03-16_103946_314-md.png 768w"><figcaption>物体精修去瑕</figcaption></figure></div><footer><div class="content__tags-share"><ul class="content__tag"><li><a href="https://dongsheng0123.github.io/tags/agi/">AGI</a></li></ul></div><div class="content__bio bio"><img src="https://dongsheng0123.github.io/media/website/Wei-Xin-Tu-Pian_2025-03-10_230135_672-2.jpg" loading="lazy" height="940" width="940" class="bio__avatar" alt="刘东升"><div><h3 class="h6 bio__name"><a href="https://dongsheng0123.github.io/liudongsheng/liu-dong-sheng/" class="invert" title="刘东升">刘东升</a></h3><div class="bio__desc"><p>一个无聊的设计师</p></div></div></div></footer></div></article></div></main><footer class="footer"><div class="footer__copyright">Designer bt 刘东升，Powered by Publii</div></footer><script>window.publiiThemeMenuConfig = {    
      mobileMenuMode: 'sidebar',
      animationSpeed: 300,
      submenuWidth: 'auto',
      doubleClickTime: 500,
      mobileMenuExpandableSubmenus: true, 
      relatedContainerForOverlayMenuSelector: '.navbar',
   };</script><script defer="defer" src="https://dongsheng0123.github.io/assets/js/scripts.min.js?v=1b09dc64eb6086ddc9720890661ff038"></script><script>var images = document.querySelectorAll('img[loading]');

      for (var i = 0; i < images.length; i++) {
         if (images[i].complete) {
               images[i].classList.add('is-loaded');
         } else {
               images[i].addEventListener('load', function () {
                  this.classList.add('is-loaded');
               }, false);
         }
      }</script></body></html>